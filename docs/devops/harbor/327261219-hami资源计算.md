---
kind:
  - Troubleshooting
products:
  - Alauda Container Platform
  - Alauda DevOps
  - Alauda AI
  - Alauda Application Services
  - Alauda Service Mesh
  - Alauda Developer Portal
ProductsVersion:
  - 4.1.0,4.2.x
---
<!-- A type of document that involves encountering a fault, diagnosing it, performing root cause analysis, and providing solutions. -->

# hami资源计算

命名空间显示GPU剩余1个但资源已全被占用 部署申请50%算力导致整卡资源被标记为耗尽 显存分配机制导致单卡显存占满后无法启动新服务

## Cause
- Hami资源计算算法与Kubernetes资源配额机制不一致
- gpucores参数被系统识别为整卡资源占用
- 显存分配采用单卡独立计算机制

## Resolution
- 按照规范配置资源申请参数：
resources:
  limits:
    nvidia.com/gpu: 1
    nvidia.com/gpumem: 3000
- 调整命名空间GPU资源配额至充足状态
- 等待算法层修复资源计算逻辑

## [workaround]
- 临时调高命名空间GPU配额
- 显存申请值预留buffer空间

## [Related Information]
**Screenshots**
![](assets/wei-fu-wu-ai-hamizi-yuan-ji-suan-112690/1752126258_99781_7bc1fe_%25E9%2583%25A8%25E7%25BD%25B2_1.png)![](assets/wei-fu-wu-ai-hamizi-yuan-ji-suan-112690/1752126259_99781_8ba79a_%25E5%2591%25BD%25E5%2590%258D%25E7%25A9%25BA%25E9%2597%25B4_1.png)
- Environment: Kubernetes 3.18.1
- nvidia.com/gpu
- nvidia.com/gpumem
- nvidia.com/gpualloc
- nvidia.com/gpucores
- Kubernetes资源配额
- Component: harbor
- Page ID: 327261219
- Original Title: 微服务-AI-hami资源计算-112690
